{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T10:22:22.579698Z",
     "start_time": "2023-05-31T10:22:22.545387Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "import PyPDF2\n",
    "\n",
    "\n",
    "def split_pdf_pages(pdf_data):\n",
    "    pages = []\n",
    "    reader = PyPDF2.PdfFileReader(io.BytesIO(pdf_data))\n",
    "\n",
    "    for page_number in range(reader.getNumPages()):\n",
    "        if reader.getPage(page_number):\n",
    "            pages.append(reader.getPage(page_number))\n",
    "\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T10:22:22.594515Z",
     "start_time": "2023-05-31T10:22:22.555548Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://ripe-mat-a91.notion.site/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F975560bb-0b88-4ee1-b226-c3310620d951%2FOmgevingsvisie_TILBURG_2040_-_01-09-2015.pdf?id=6e0fc059-2dcb-4bcb-b7ff-eecb99237227&table=block&spaceId=ca274338-dfb2-4f65-823d-2acde6195fea&name=Omgevingsvisie%20TILBURG%202040%20-%2001-09-2015.pdf&cache=v2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T10:22:22.641276Z",
     "start_time": "2023-05-31T10:22:22.570701Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "\n",
    "def extract_title(url):\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "    query_params = urllib.parse.parse_qs(parsed_url.query)\n",
    "    download_name = query_params.get(\"downloadName\", [None])[0]\n",
    "    return download_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T10:22:22.642269Z",
     "start_time": "2023-05-31T10:22:22.586517Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def count_keywords_in_string(string):\n",
    "    keyword_count = 0\n",
    "    for keyword in keywords:\n",
    "        keyword_count += string.lower().count(keyword.lower())\n",
    "    return keyword_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T10:22:33.746106Z",
     "start_time": "2023-05-31T10:22:22.606150Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "anton = []\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    pdf_data = response.content\n",
    "    pdf_pages = split_pdf_pages(pdf_data)\n",
    "    for page_number, page in enumerate(pdf_pages):\n",
    "        contents = page.extract_text()\n",
    "        anton.append(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T10:22:44.541754Z",
     "start_time": "2023-05-31T10:22:33.749106Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'iets', 'wiens', 'ieder', 'enkele', 'dezelfde', 'we', 'volgens', 'bovendien', 'eveneens', 'ja', 'boven', 'zo', 'daarheen', 'moesten', 'des', 'hierboven', 'af', 'alle', 'heeft', 'nog', 'net', 'hem', 'uit', 'zekere', 'hierin', 'want', 'veel', 'waar', 'onze', 'doen', 'rond', 'je', 'geworden', 'vandaan', 'bent', 'thans', 'vanaf', 'weldra', 'na', 'zelfs', 'voort', 'welke', 'zichzelf', 'dit', 'daarom', 'ongeveer', 'mede', 'er', 'eigen', 'die', 'zult', 'beter', 'zulks', 'eens', 'allebei', 'zullen', 'nabij', 'mochten', 'gauw', 'opnieuw', 'steeds', 'daarna', 'ff', 'allen', 'dat', 'hebt', 'jou', 'weinige', 'sinds', 'eersten', 'weg', 'zeer', 'enige', 'omhoog', 'beneden', 'omdat', 'zijn', 'geleden', 'zeker', 'werd', 'andere', 'over', 'kunt', 'hadden', 'elk', 'uw', 'hele', 'zij', 'hun', 'voorbij', 'voor', 'vooraf', 'bijvoorbeeld', 'omlaag', 'gewoon', 'aangaangde', 'daarin', 'me', 'zal', 'het', 'onder', 'haar', 'ten', 'omver', 'geweest', 'geven', 'beiden', 'daarop', 'bovenstaand', 'als', 'tot', 'is', 'wij', 'worden', 'doorgaand', 'mezelf', 'vroeg', 'wat', 'of', 'hoewel', 'spoedig', 'van', 'u', 'hierbeneden', 'bepaald', 'onszelf', 'nr', 'meesten', 'zijnde', 'voorheen', 'enz', 'behalve', 'gemogen', 'jullie', 'met', 'reeds', 'effe', 'ook', 'deze', 'moet', 'op', 'heb', 'overigens', 'zelf', 'zodra', 'misschien', 'opzij', 'vooral', 'te', 'ze', 'inzake', 'evenwel', 'ik', 'wegens', 'gewoonweg', 'welken', 'wier', 'doch', 'ondertussen', 'en', 'vrij', 'inz', 'voorts', 'terwijl', 'gekund', 'de', 'publ', 'dikwijls', 'pp', 'idd', 'niets', 'dus', 'had', 'om', 'zouden', 'zelfde', 'inmiddels', 'vooruit', 'mijzelf', 'men', 'toch', 'opdat', 'anderzijds', 'mag', 'hen', 'toen', 'ge', 'juist', 'nu', 'daar', 'ikzelf', 'kon', 'dezen', 'wel', 'waarom', 'alles', 'moest', 'in', 'minder', 'mocht', 'ander', 'voordat', 'tamelijk', 'maar', 'nogal', 'ooit', 'binnenin', 'enkel', 'uwe', 'hebben', 'uitgezonderd', 'uwen', 'veeleer', 'eerst', 'anderen', 'een', 'geen', 'anders', 'doet', 'vgl', 'niet', 'moeten', 'jij', 'voordien', 'buiten', 'ben', 'mijn', 'altijd', 'elke', 'betere', 'der', 'nooit', 'vanwege', 'door', 'sommige', 'waren', 'zulke', 'vervolgens', 'afgelopen', 'dien', 'alleen', 'sindsdien', 'erdoor', 'krachtens', 'gij', 'weer', 'bij', 'wanneer', 'gegeven', 'achterna', \"'t\", 'verder', 'weinig', 'omtrent', 'wil', 'aangezien', 'vanuit', 'min', 'werden', 'beide', 'ter', 'echter', 'slechts', 'zou', 'aldus', 'achter', 'even', 'jezelf', 'ikke', 'den', 'gehad', 'vooralsnog', 'wilde', 'naar', 'jijzelf', 'doorgaans', 'bijna', 'hoe', 'wie', 'jouw', 'totdat', 'bovenal', 'precies', 'mogen', 'betreffende', 'wezen', 'binnen', 'zoveel', 'kunnen', 'meer', 'zonder', 'hier', 'hare', 'kan', 'indien', 'daarnet', 'hij', 'alhoewel', 'dan', 'liever', 'tegen', 'zo’n', 'lang', 'rondom', 'zei', 'welk', 'verre', 'tijdens', 'toe', 'was', 'voorop', 'gedurende', 'aan', 'later', 'zoals', 'klaar', 'al', 'eerste', 'zijne', 'tussen', 'zich', 'wordt', 'prof', 'konden', '‘t', 'pas', 'nadat', 'ons', 'sedert', 'mogelijk', 'gelijk', 'etc', 'iemand', 'mij', 'liet', 'zowat', 'vaak', 'zulk', 'eerder', 'omstreeks', 'tenzij', 'geheel', 'jouwe'} instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 28\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     27\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39mnlp\u001b[38;5;241m.\u001b[39mDefaults\u001b[38;5;241m.\u001b[39mstop_words, tokenizer\u001b[38;5;241m=\u001b[39mtokenize_text, preprocessor\u001b[38;5;241m=\u001b[39mpreprocess_text)\n\u001b[1;32m---> 28\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43manton\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Step 4: Determine the most important keywords from the TF-IDF analysis\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Users\\luc\\anaconda3\\envs\\sem4_base\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2121\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2116\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2117\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2118\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2119\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2120\u001b[0m )\n\u001b[1;32m-> 2121\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2123\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2124\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Users\\luc\\anaconda3\\envs\\sem4_base\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1358\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_documents, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1354\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1355\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterable over raw text documents expected, string object received.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1356\u001b[0m     )\n\u001b[1;32m-> 1358\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_ngram_range()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n",
      "File \u001b[1;32mD:\\Users\\luc\\anaconda3\\envs\\sem4_base\\lib\\site-packages\\sklearn\\base.py:570\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Users\\luc\\anaconda3\\envs\\sem4_base\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:97\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m constraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m     )\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'iets', 'wiens', 'ieder', 'enkele', 'dezelfde', 'we', 'volgens', 'bovendien', 'eveneens', 'ja', 'boven', 'zo', 'daarheen', 'moesten', 'des', 'hierboven', 'af', 'alle', 'heeft', 'nog', 'net', 'hem', 'uit', 'zekere', 'hierin', 'want', 'veel', 'waar', 'onze', 'doen', 'rond', 'je', 'geworden', 'vandaan', 'bent', 'thans', 'vanaf', 'weldra', 'na', 'zelfs', 'voort', 'welke', 'zichzelf', 'dit', 'daarom', 'ongeveer', 'mede', 'er', 'eigen', 'die', 'zult', 'beter', 'zulks', 'eens', 'allebei', 'zullen', 'nabij', 'mochten', 'gauw', 'opnieuw', 'steeds', 'daarna', 'ff', 'allen', 'dat', 'hebt', 'jou', 'weinige', 'sinds', 'eersten', 'weg', 'zeer', 'enige', 'omhoog', 'beneden', 'omdat', 'zijn', 'geleden', 'zeker', 'werd', 'andere', 'over', 'kunt', 'hadden', 'elk', 'uw', 'hele', 'zij', 'hun', 'voorbij', 'voor', 'vooraf', 'bijvoorbeeld', 'omlaag', 'gewoon', 'aangaangde', 'daarin', 'me', 'zal', 'het', 'onder', 'haar', 'ten', 'omver', 'geweest', 'geven', 'beiden', 'daarop', 'bovenstaand', 'als', 'tot', 'is', 'wij', 'worden', 'doorgaand', 'mezelf', 'vroeg', 'wat', 'of', 'hoewel', 'spoedig', 'van', 'u', 'hierbeneden', 'bepaald', 'onszelf', 'nr', 'meesten', 'zijnde', 'voorheen', 'enz', 'behalve', 'gemogen', 'jullie', 'met', 'reeds', 'effe', 'ook', 'deze', 'moet', 'op', 'heb', 'overigens', 'zelf', 'zodra', 'misschien', 'opzij', 'vooral', 'te', 'ze', 'inzake', 'evenwel', 'ik', 'wegens', 'gewoonweg', 'welken', 'wier', 'doch', 'ondertussen', 'en', 'vrij', 'inz', 'voorts', 'terwijl', 'gekund', 'de', 'publ', 'dikwijls', 'pp', 'idd', 'niets', 'dus', 'had', 'om', 'zouden', 'zelfde', 'inmiddels', 'vooruit', 'mijzelf', 'men', 'toch', 'opdat', 'anderzijds', 'mag', 'hen', 'toen', 'ge', 'juist', 'nu', 'daar', 'ikzelf', 'kon', 'dezen', 'wel', 'waarom', 'alles', 'moest', 'in', 'minder', 'mocht', 'ander', 'voordat', 'tamelijk', 'maar', 'nogal', 'ooit', 'binnenin', 'enkel', 'uwe', 'hebben', 'uitgezonderd', 'uwen', 'veeleer', 'eerst', 'anderen', 'een', 'geen', 'anders', 'doet', 'vgl', 'niet', 'moeten', 'jij', 'voordien', 'buiten', 'ben', 'mijn', 'altijd', 'elke', 'betere', 'der', 'nooit', 'vanwege', 'door', 'sommige', 'waren', 'zulke', 'vervolgens', 'afgelopen', 'dien', 'alleen', 'sindsdien', 'erdoor', 'krachtens', 'gij', 'weer', 'bij', 'wanneer', 'gegeven', 'achterna', \"'t\", 'verder', 'weinig', 'omtrent', 'wil', 'aangezien', 'vanuit', 'min', 'werden', 'beide', 'ter', 'echter', 'slechts', 'zou', 'aldus', 'achter', 'even', 'jezelf', 'ikke', 'den', 'gehad', 'vooralsnog', 'wilde', 'naar', 'jijzelf', 'doorgaans', 'bijna', 'hoe', 'wie', 'jouw', 'totdat', 'bovenal', 'precies', 'mogen', 'betreffende', 'wezen', 'binnen', 'zoveel', 'kunnen', 'meer', 'zonder', 'hier', 'hare', 'kan', 'indien', 'daarnet', 'hij', 'alhoewel', 'dan', 'liever', 'tegen', 'zo’n', 'lang', 'rondom', 'zei', 'welk', 'verre', 'tijdens', 'toe', 'was', 'voorop', 'gedurende', 'aan', 'later', 'zoals', 'klaar', 'al', 'eerste', 'zijne', 'tussen', 'zich', 'wordt', 'prof', 'konden', '‘t', 'pas', 'nadat', 'ons', 'sedert', 'mogelijk', 'gelijk', 'etc', 'iemand', 'mij', 'liet', 'zowat', 'vaak', 'zulk', 'eerder', 'omstreeks', 'tenzij', 'geheel', 'jouwe'} instead."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "nlp = spacy.load(\"nl_core_news_lg\")\n",
    "punctuations = string.punctuation\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nlp(str(text))\n",
    "    keywords = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token.is_punct or token.is_currency:\n",
    "            continue\n",
    "        elif token.pos_ in [\"NOUN\", \"PROPN\", \"ADJ\"]:\n",
    "            keywords.append(token.lemma_)\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, tuple):\n",
    "        text = ' '.join(str(t) for t in text)\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=nlp.Defaults.stop_words, tokenizer=tokenize_text, preprocessor=preprocess_text)\n",
    "tfidf_matrix = vectorizer.fit_transform(anton)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Step 4: Determine the most important keywords from the TF-IDF analysis\n",
    "num_keywords = 1  # Adjust the number of keywords you want to extract\n",
    "useful_keywords = []\n",
    "for i, document in enumerate(anton):\n",
    "    tfidf_scores = tfidf_matrix[i].toarray()[0]\n",
    "    top_indices = tfidf_scores.argsort()[-num_keywords:][::-1]\n",
    "    top_keywords = [feature_names[idx] for idx in top_indices]\n",
    "    useful_keywords.extend(top_keywords)\n",
    "\n",
    "useful_keywords = list(set(useful_keywords))  # Remove duplicate keywords\n",
    "useful_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T10:22:55.188247Z",
     "start_time": "2023-05-31T10:22:44.545757Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keywords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 60\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m count\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Create the pipeline\u001b[39;00m\n\u001b[0;32m     58\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     59\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf_extractor\u001b[39m\u001b[38;5;124m'\u001b[39m, PDFPageExtractor()),\n\u001b[1;32m---> 60\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage_filter\u001b[39m\u001b[38;5;124m'\u001b[39m, PageContentFilter(\u001b[43mkeywords\u001b[49m))\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m ])\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Apply the pipeline to the URLs\u001b[39;00m\n\u001b[0;32m     65\u001b[0m filtered_pages2 \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mtransform(urls)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keywords' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from PyPDF2 import PdfFileReader\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "class PDFPageExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        extracted_pages = []\n",
    "        for url in X:\n",
    "            response = requests.get(url)\n",
    "            pdf_data = response.content\n",
    "            pdf_pages = self.split_pdf_pages(pdf_data)\n",
    "            extracted_pages.extend(pdf_pages)\n",
    "        return extracted_pages\n",
    "\n",
    "    def split_pdf_pages(self, pdf_data):\n",
    "        pdf = PdfFileReader(BytesIO(pdf_data))\n",
    "        return [pdf.getPage(i) for i in range(pdf.getNumPages())]\n",
    "\n",
    "\n",
    "class PageContentFilter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keywords, max_keyword_count=20):\n",
    "        self.keywords = keywords\n",
    "        self.max_keyword_count = max_keyword_count\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        filtered_pages = []\n",
    "        for page_number, page in enumerate(X):\n",
    "            contents = page.extract_text()\n",
    "            if contents == \"\" or contents == \" \":\n",
    "                print(f\"BLANK {page_number}\")\n",
    "                continue\n",
    "            if not any(keyword in contents for keyword in self.keywords):\n",
    "                print(f\"Not any keywords found {page_number}\")\n",
    "                continue\n",
    "            if self.count_keywords_in_string(contents) <= self.max_keyword_count:\n",
    "                print(f\"Not enough keywords found {page_number}\")\n",
    "                continue\n",
    "            filtered_pages.append((page_number, contents))\n",
    "        return filtered_pages\n",
    "\n",
    "    def count_keywords_in_string(self, text):\n",
    "        count = 0\n",
    "        for keyword in self.keywords:\n",
    "            count += text.lower().count(keyword.lower())\n",
    "        return count\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('pdf_extractor', PDFPageExtractor()),\n",
    "    ('page_filter', PageContentFilter(keywords))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the URLs\n",
    "filtered_pages2 = pipeline.transform(urls)\n",
    "\n",
    "#Print the filtered pages\n",
    "# for page_number, contents in filtered_pages2:\n",
    "#     print(\"Page number:\", page_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T10:22:55.208293Z",
     "start_time": "2023-05-31T10:22:55.189239Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_pages2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfiltered_pages2\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filtered_pages2' is not defined"
     ]
    }
   ],
   "source": [
    "filtered_pages2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
