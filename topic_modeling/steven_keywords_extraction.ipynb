{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-31T10:22:22.545387Z",
     "end_time": "2023-05-31T10:22:22.579698Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "import PyPDF2\n",
    "\n",
    "\n",
    "def split_pdf_pages(pdf_data):\n",
    "    pages = []\n",
    "    reader = PyPDF2.PdfFileReader(io.BytesIO(pdf_data))\n",
    "\n",
    "    for page_number in range(reader.getNumPages()):\n",
    "        if reader.getPage(page_number):\n",
    "            pages.append(reader.getPage(page_number))\n",
    "\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://ripe-mat-a91.notion.site/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F975560bb-0b88-4ee1-b226-c3310620d951%2FOmgevingsvisie_TILBURG_2040_-_01-09-2015.pdf?id=6e0fc059-2dcb-4bcb-b7ff-eecb99237227&table=block&spaceId=ca274338-dfb2-4f65-823d-2acde6195fea&name=Omgevingsvisie%20TILBURG%202040%20-%2001-09-2015.pdf&cache=v2\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-31T10:22:22.555548Z",
     "end_time": "2023-05-31T10:22:22.594515Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "\n",
    "def extract_title(url):\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "    query_params = urllib.parse.parse_qs(parsed_url.query)\n",
    "    download_name = query_params.get(\"downloadName\", [None])[0]\n",
    "    return download_name"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-31T10:22:22.570701Z",
     "end_time": "2023-05-31T10:22:22.641276Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def count_keywords_in_string(string):\n",
    "    keyword_count = 0\n",
    "    for keyword in keywords:\n",
    "        keyword_count += string.lower().count(keyword.lower())\n",
    "    return keyword_count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-31T10:22:22.586517Z",
     "end_time": "2023-05-31T10:22:22.642269Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "anton = []\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    pdf_data = response.content\n",
    "    pdf_pages = split_pdf_pages(pdf_data)\n",
    "    for page_number, page in enumerate(pdf_pages):\n",
    "        contents = page.extract_text()\n",
    "        anton.append(contents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-31T10:22:22.606150Z",
     "end_time": "2023-05-31T10:22:33.746106Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "['ondergrond',\n 'interpolisnieuw',\n '42bijlage',\n 'herstructurering',\n 'spoor',\n 'noord',\n 'spindern261',\n 'brabantstrategie',\n 'regenwater',\n 'water',\n '38bijlage',\n 'landschap',\n 'locatie',\n 'kwadrant',\n 'motion',\n 'schaalniveau',\n 'burger',\n 'wijkevoort',\n 'vestigingenregister',\n 'moerenburg',\n 'a58',\n 'figuur',\n '90regiostrategie',\n 'camera',\n 'wijk',\n 'regiostrategie',\n 'categorieeenmanszaak2',\n 'parkmoerenburg',\n '44bijlage',\n 'haghorstmolenschotdorsttuinbouw',\n 'afweegbaar',\n 'kapitaal',\n 'september',\n 'bijlag',\n 'bossportlanessportcomplexwerklandschapwaterpark',\n 'categorie',\n 'nieuw',\n '21',\n 'gemeente',\n 'inleiding1',\n 'kantoor',\n 'systeem',\n 'bijlage',\n 'reeshof',\n 'binnenstad',\n 'industrie',\n 'stadsgezicht',\n 'netwerk',\n 'inhoudsopgave',\n 'midden-brabant',\n 'hoogbouw',\n 'logistiek',\n '’s',\n 'bewonersadvie',\n 'stadsstrategie',\n 'regionaal',\n 'aantal',\n 'moti',\n 'stadsbos013',\n 'regeling',\n 'tilburg',\n 'opslag',\n 'energie',\n 'openbaar',\n 'Eeuws',\n 'recreatie',\n 'buurt',\n 'sportlanes',\n 'sector',\n 'food',\n 'duurzaamheidsbalans',\n 'circulair',\n 'wet',\n 'leefomgeving',\n 'partij',\n 'noord-brabant',\n 'instrumentarium',\n 'natuur',\n 'stadsregionaal',\n 'fontys',\n 'leij',\n 'voertuig',\n 'tt2040',\n 'park',\n 'hoofdstuk',\n 'afstand',\n 'industriespec',\n 'wereld',\n 'snelfietsroute',\n 'groen',\n 'plandeel',\n 'zorg',\n 'dienstverleningwater',\n 'urbanen',\n 'biodiversiteit',\n 'internethandel',\n 'tilburger',\n 'midden-brabantweg',\n 'ontwikkelingen2',\n 'westkamerhoogspanningstracé',\n 'leijpark',\n 'grondbeleid',\n 'omgevingsvisie',\n 'overeenkomst',\n 'smal',\n 'beleid',\n 'ziekenhuis',\n 'Brabant',\n 'bevolking',\n 'duurzaam',\n 'universiteit',\n 'loven',\n 'traject',\n 'archeologisch',\n 'bijzonder',\n 'onderzoek',\n 'ecologisch',\n 'kanaalzone',\n 'mobiliteit',\n 'leija58-knoopkempenbaanzorgcluster',\n 'meteconomisch',\n '26bijlage',\n 'woning',\n 'samenvatting',\n 'accent',\n 'stad']"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "punctuations = string.punctuation\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nlp(str(text))\n",
    "    keywords = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token.is_punct or token.is_currency:\n",
    "            continue\n",
    "        elif token.pos_ in [\"NOUN\", \"PROPN\", \"ADJ\"]:\n",
    "            keywords.append(token.lemma_)\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, tuple):\n",
    "        text = ' '.join(str(t) for t in text)\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=nlp.Defaults.stop_words, tokenizer=tokenize_text, preprocessor=preprocess_text)\n",
    "tfidf_matrix = vectorizer.fit_transform(anton)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Step 4: Determine the most important keywords from the TF-IDF analysis\n",
    "num_keywords = 1  # Adjust the number of keywords you want to extract\n",
    "useful_keywords = []\n",
    "for i, document in enumerate(anton):\n",
    "    tfidf_scores = tfidf_matrix[i].toarray()[0]\n",
    "    top_indices = tfidf_scores.argsort()[-num_keywords:][::-1]\n",
    "    top_keywords = [feature_names[idx] for idx in top_indices]\n",
    "    useful_keywords.extend(top_keywords)\n",
    "    # top_weights = [tfidf_scores[idx] for idx in top_indices]\n",
    "    #\n",
    "    # for keyword, weight in zip(top_keywords, top_weights):\n",
    "    #     print(\"Keyword:\", keyword)\n",
    "    #     print(\"Weight:\", weight)\n",
    "    #     print(\"---\")\n",
    "\n",
    "useful_keywords = list(set(useful_keywords))  # Remove duplicate keywords\n",
    "keywords = useful_keywords\n",
    "useful_keywords"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-31T10:22:33.749106Z",
     "end_time": "2023-05-31T10:22:44.541754Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough keywords found 0\n",
      "Not any keywords found 1\n",
      "Not any keywords found 3\n",
      "Not any keywords found 10\n",
      "Not any keywords found 15\n",
      "Not enough keywords found 17\n",
      "Not enough keywords found 19\n",
      "Not enough keywords found 35\n",
      "Not enough keywords found 38\n",
      "Not enough keywords found 45\n",
      "Not any keywords found 51\n",
      "Not any keywords found 53\n",
      "Not enough keywords found 57\n",
      "Not enough keywords found 59\n",
      "Not enough keywords found 61\n",
      "Not enough keywords found 65\n",
      "Not enough keywords found 69\n",
      "Not any keywords found 73\n",
      "Not enough keywords found 75\n",
      "Not enough keywords found 78\n",
      "Not enough keywords found 79\n",
      "Not enough keywords found 81\n",
      "Not enough keywords found 85\n",
      "Not enough keywords found 89\n",
      "Not enough keywords found 91\n",
      "Not enough keywords found 93\n",
      "Not enough keywords found 95\n",
      "Not enough keywords found 97\n",
      "Not any keywords found 99\n",
      "Not any keywords found 101\n",
      "Not any keywords found 109\n",
      "Not enough keywords found 119\n",
      "Not enough keywords found 124\n",
      "Not enough keywords found 125\n",
      "Not any keywords found 126\n",
      "Not any keywords found 133\n",
      "Not enough keywords found 134\n",
      "BLANK 135\n",
      "Not enough keywords found 139\n",
      "Not enough keywords found 142\n",
      "Not enough keywords found 145\n",
      "Not enough keywords found 149\n",
      "Not enough keywords found 150\n",
      "Not enough keywords found 151\n",
      "Not enough keywords found 152\n",
      "Not enough keywords found 153\n",
      "Not enough keywords found 154\n",
      "Not any keywords found 155\n",
      "Not enough keywords found 156\n",
      "Not enough keywords found 157\n",
      "Not any keywords found 158\n",
      "Not enough keywords found 159\n",
      "Not enough keywords found 160\n",
      "Not any keywords found 161\n",
      "Not enough keywords found 162\n",
      "Not enough keywords found 164\n",
      "Not any keywords found 165\n",
      "Not any keywords found 166\n",
      "Not enough keywords found 167\n",
      "Not enough keywords found 168\n",
      "Not any keywords found 170\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from PyPDF2 import PdfFileReader\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "class PDFPageExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        extracted_pages = []\n",
    "        for url in X:\n",
    "            response = requests.get(url)\n",
    "            pdf_data = response.content\n",
    "            pdf_pages = self.split_pdf_pages(pdf_data)\n",
    "            extracted_pages.extend(pdf_pages)\n",
    "        return extracted_pages\n",
    "\n",
    "    def split_pdf_pages(self, pdf_data):\n",
    "        pdf = PdfFileReader(BytesIO(pdf_data))\n",
    "        return [pdf.getPage(i) for i in range(pdf.getNumPages())]\n",
    "\n",
    "\n",
    "class PageContentFilter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keywords, max_keyword_count=20):\n",
    "        self.keywords = keywords\n",
    "        self.max_keyword_count = max_keyword_count\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        filtered_pages = []\n",
    "        for page_number, page in enumerate(X):\n",
    "            contents = page.extract_text()\n",
    "            if contents == \"\" or contents == \" \":\n",
    "                print(f\"BLANK {page_number}\")\n",
    "                continue\n",
    "            if not any(keyword in contents for keyword in self.keywords):\n",
    "                print(f\"Not any keywords found {page_number}\")\n",
    "                continue\n",
    "            if self.count_keywords_in_string(contents) <= self.max_keyword_count:\n",
    "                print(f\"Not enough keywords found {page_number}\")\n",
    "                continue\n",
    "            filtered_pages.append((page_number, contents))\n",
    "        return filtered_pages\n",
    "\n",
    "    def count_keywords_in_string(self, text):\n",
    "        count = 0\n",
    "        for keyword in self.keywords:\n",
    "            count += text.lower().count(keyword.lower())\n",
    "        return count\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('pdf_extractor', PDFPageExtractor()),\n",
    "    ('page_filter', PageContentFilter(keywords))\n",
    "\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the URLs\n",
    "filtered_pages2 = pipeline.transform(urls)\n",
    "\n",
    "#Print the filtered pages\n",
    "# for page_number, contents in filtered_pages2:\n",
    "#     print(\"Page number:\", page_number)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-31T10:22:44.545757Z",
     "end_time": "2023-05-31T10:22:55.188247Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-31T10:22:55.189239Z",
     "end_time": "2023-05-31T10:22:55.208293Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
