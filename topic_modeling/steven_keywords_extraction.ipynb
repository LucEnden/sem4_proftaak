{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-25T14:24:12.634424Z",
     "end_time": "2023-05-25T14:24:12.647423Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "import PyPDF2\n",
    "\n",
    "\n",
    "def split_pdf_pages(pdf_data):\n",
    "    pages = []\n",
    "    reader = PyPDF2.PdfFileReader(io.BytesIO(pdf_data))\n",
    "\n",
    "    for page_number in range(reader.getNumPages()):\n",
    "        if reader.getPage(page_number):\n",
    "            pages.append(reader.getPage(page_number))\n",
    "\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://file.notion.so/f/s/70234c16-e61d-48fc-a26e-a2a573afaee1/Woonzorgvisie_Gemeente_Oisterwijk_2022_-_2027_-_02-05-2022.pdf?id=387149cf-e0c7-4f5f-9b76-250ac04d8e9d&table=block&spaceId=ca274338-dfb2-4f65-823d-2acde6195fea&expirationTimestamp=1685090661894&signature=SZ8bKZJmrGVF9wLYb_eZkI7Ozgmssh7TmBFyTzw1T5E&downloadName=Woonzorgvisie+Gemeente+Oisterwijk+2022+-+2027+-+02-05-2022.pdf\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-25T14:24:12.727309Z",
     "end_time": "2023-05-25T14:24:12.740329Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "\n",
    "def extract_title(url):\n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "    query_params = urllib.parse.parse_qs(parsed_url.query)\n",
    "    download_name = query_params.get(\"downloadName\", [None])[0]\n",
    "    return download_name"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-25T14:24:12.879589Z",
     "end_time": "2023-05-25T14:24:12.896854Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "def count_keywords_in_string(string):\n",
    "    keyword_count = 0\n",
    "    for keyword in keywords:\n",
    "        keyword_count += string.lower().count(keyword.lower())\n",
    "    return keyword_count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-25T14:24:13.106945Z",
     "end_time": "2023-05-25T14:24:13.112133Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "anton = []\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    pdf_data = response.content\n",
    "    pdf_pages = split_pdf_pages(pdf_data)\n",
    "    for page_number, page in enumerate(pdf_pages):\n",
    "        contents = page.extract_text()\n",
    "        anton.append(contents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-25T14:24:13.261977Z",
     "end_time": "2023-05-25T14:24:18.650181Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "['aanpak',\n 'urgentie',\n 'jaar',\n 'landschap',\n 'inschrijftijd',\n 'matchingpunt',\n 'karakter',\n 'entameren',\n 'omgevingsvisie',\n 'centrum',\n 'waterhoef',\n 'plan',\n 'koop',\n 'inkomen',\n 'vergunninghouders',\n 'kerkhof',\n 'werking',\n 'beperking',\n 'mantelzorg',\n 'egw',\n 'heukelom',\n 'betaalbaar',\n 'één',\n 'lokaal',\n 'bruto',\n 'begrip',\n 'tijdelijk',\n 'bijlage',\n 'dorp',\n 'wijk',\n 'standplaat',\n 'woningtype',\n 'groen',\n 'overeenkomst',\n 'oisterwijk',\n 'eengezinswoning',\n 'sociaal',\n 'haaren',\n 'creatief',\n 'integraal',\n 'woning',\n 'ruimte',\n 'titel',\n 'visie',\n 'oud',\n 'regionaal',\n 'voorontwerp']"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "punctuations = string.punctuation\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nlp(str(text))\n",
    "    keywords = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token.is_punct or token.is_currency:\n",
    "            continue\n",
    "        elif token.pos_ in [\"NOUN\", \"PROPN\", \"ADJ\"]:\n",
    "            keywords.append(token.lemma_)\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, tuple):\n",
    "        text = ' '.join(str(t) for t in text)\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=nlp.Defaults.stop_words, tokenizer=tokenize_text, preprocessor=preprocess_text)\n",
    "tfidf_matrix = vectorizer.fit_transform(anton)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Step 4: Determine the most important keywords from the TF-IDF analysis\n",
    "num_keywords = 1  # Adjust the number of keywords you want to extract\n",
    "useful_keywords = []\n",
    "for i, document in enumerate(anton):\n",
    "    tfidf_scores = tfidf_matrix[i].toarray()[0]\n",
    "    top_indices = tfidf_scores.argsort()[-num_keywords:][::-1]\n",
    "    top_keywords = [feature_names[idx] for idx in top_indices]\n",
    "    useful_keywords.extend(top_keywords)\n",
    "    # top_weights = [tfidf_scores[idx] for idx in top_indices]\n",
    "    #\n",
    "    # for keyword, weight in zip(top_keywords, top_weights):\n",
    "    #     print(\"Keyword:\", keyword)\n",
    "    #     print(\"Weight:\", weight)\n",
    "    #     print(\"---\")\n",
    "\n",
    "useful_keywords = list(set(useful_keywords))  # Remove duplicate keywords\n",
    "keywords = useful_keywords\n",
    "useful_keywords"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-25T14:24:18.654179Z",
     "end_time": "2023-05-25T14:24:24.815888Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough keywords found 0\n",
      "Not enough keywords found 1\n",
      "Not enough keywords found 2\n",
      "Not enough keywords found 4\n",
      "Not enough keywords found 9\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from PyPDF2 import PdfFileReader\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "class PDFPageExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        extracted_pages = []\n",
    "        for url in X:\n",
    "            response = requests.get(url)\n",
    "            pdf_data = response.content\n",
    "            pdf_pages = self.split_pdf_pages(pdf_data)\n",
    "            extracted_pages.extend(pdf_pages)\n",
    "        return extracted_pages\n",
    "\n",
    "    def split_pdf_pages(self, pdf_data):\n",
    "        pdf = PdfFileReader(BytesIO(pdf_data))\n",
    "        return [pdf.getPage(i) for i in range(pdf.getNumPages())]\n",
    "\n",
    "\n",
    "class PageContentFilter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keywords, max_keyword_count=20):\n",
    "        self.keywords = keywords\n",
    "        self.max_keyword_count = max_keyword_count\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        filtered_pages = []\n",
    "        for page_number, page in enumerate(X):\n",
    "            contents = page.extract_text()\n",
    "            if contents == \"\" or contents == \" \":\n",
    "                print(f\"BLANK {page_number}\")\n",
    "                continue\n",
    "            if not any(keyword in contents for keyword in self.keywords):\n",
    "                print(f\"Not any keywords found {page_number}\")\n",
    "                continue\n",
    "            if self.count_keywords_in_string(contents) <= self.max_keyword_count:\n",
    "                print(f\"Not enough keywords found {page_number}\")\n",
    "                continue\n",
    "            filtered_pages.append((page_number, contents))\n",
    "        return filtered_pages\n",
    "\n",
    "    def count_keywords_in_string(self, text):\n",
    "        count = 0\n",
    "        for keyword in self.keywords:\n",
    "            count += text.lower().count(keyword.lower())\n",
    "        return count\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('pdf_extractor', PDFPageExtractor()),\n",
    "    ('page_filter', PageContentFilter(keywords))\n",
    "\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the URLs\n",
    "filtered_pages2 = pipeline.transform(urls)\n",
    "\n",
    "#Print the filtered pages\n",
    "# for page_number, contents in filtered_pages2:\n",
    "#     print(\"Page number:\", page_number)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-25T14:15:02.184067Z",
     "end_time": "2023-05-25T14:15:07.628964Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
