{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "825b98d9-8301-4bd6-867c-daa09ff646ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import spacy\n",
    "import re\n",
    "from bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper, ContinuousColorMapper, LinearColorMapper, ColorBar, LabelSet, Label\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from gensim.models import KeyedVectors\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "import requests\n",
    "import pandas as pd\n",
    "from gensim.test.utils import common_texts\n",
    "import gensim.downloader\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from matplotlib import pyplot as plt\n",
    "from bokeh.palettes import Spectral10\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import string\n",
    "import sys\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import ipywidgets as widgets\n",
    "import umap\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4dc6f0-2a58-4df1-9f53-5fbec4c010be",
   "metadata": {},
   "source": [
    "- gegeven de distributie van woorden, wat als je alle worden weg filter welke in het middelpunt van de curve vallen (66%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90b6eb4-f0f6-4b9b-9d6a-8981d82af434",
   "metadata": {},
   "source": [
    "gegeven een lijst met keywords, hoe kunnen we topics uit de documenten halen welke van toepassing zijn op de keywords?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952efe6c-f273-409a-bfe7-cbaa228aea91",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Research question 2\n",
    "\n",
    "> *\"Welke KPI's zitten verweven in de aangeleverde overheid documenten (denk aan geplande aantal huizen bijvoorbeeld), en wat zijn de overeenkomsten tussen deze KPI's? \"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f38f74-0be3-41c4-a350-d60c87fe9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_lg\n",
    "import warnings\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "spacy_model = en_core_web_lg.load()\n",
    "\n",
    "def preprocess_phrase(phrase):\n",
    "    split_phrase = wordpunct_tokenize(phrase.lower())\n",
    "    return \" \".join([ lemmatizer.lemmatize(i) for i in split_phrase ])\n",
    "\n",
    "def semantic_similarity(X, Y, max_out=None):\n",
    "    # This function loads the large English language model in spaCy\n",
    "    # and uses it to calculate semantic similarity between each phrase in Y and the input phrase X.\n",
    "    # Note that this function assumes that the phrases in Y are in English, \n",
    "    # and it may not work as well for other languages or for very short or ungrammatical phrases.\n",
    "    \n",
    "    # Create a Doc object for X\n",
    "    doc1 = spacy_model(preprocess_phrase(X))\n",
    "\n",
    "    # Calculate semantic similarity for each phrase in Y\n",
    "    similarities = []\n",
    "    for phrase in Y:\n",
    "        doc2 = spacy_model(preprocess_phrase(phrase))\n",
    "        similarity = doc1.similarity(doc2)\n",
    "        similarities.append((doc2, similarity))\n",
    "        \n",
    "    # return similarities\n",
    "    if not max_out:\n",
    "        max_out = len(similarities)\n",
    "    return sorted(similarities, key=lambda a: a[1], reverse=True)[0:max_out]\n",
    "\n",
    "def syntactic_similarity(X, Y, max_out=None):\n",
    "    # This function takes in two parameters: \n",
    "    # X, the single phrase to compare against the list of phrases\n",
    "    # Y, the list of phrases to compare against. \n",
    "    # It returns a tuple containing the most similar phrase from Y and the corresponding syntactic similarity score.\n",
    "    match_ratios = []\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for phrase in Y:\n",
    "            matcher = SequenceMatcher(None, preprocess_phrase(X), preprocess_phrase(phrase))\n",
    "            ratio = matcher.ratio()\n",
    "            match_ratios.append((phrase, ratio))\n",
    "    \n",
    "    if not max_out:\n",
    "        max_out = len(match_ratios)\n",
    "        \n",
    "    return sorted(match_ratios, key=lambda a: a[1], reverse=True)[0:max_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcacbdda-5e9f-4c15-b260-a5c994e5a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "handmatige_kpi_collectie = pd.read_csv(\"handmatige_kpi_collectie.csv\")\n",
    "handmatige_kpi_collectie.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4703a42e-c6af-4211-9360-9adc35070650",
   "metadata": {},
   "outputs": [],
   "source": [
    "handmatige_kpi_collectie.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080b5185-237f-4cc4-93ad-96c1490cf9d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## KPI similaritie measarumnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e7020-303d-4425-9261-46addd36236d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kpis = list(handmatige_kpi_collectie.KPI.astype(str))\n",
    "# synt_syms = dict()\n",
    "# for kpi in tqdm(kpis):\n",
    "#     others = [ i for i in kpis if i != kpi ]\n",
    "#     synt_syms[kpi] = syntactic_similarity(kpi, others, 5)\n",
    "    \n",
    "# with open(\"./synt_sema.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "#     content = \"<===> KPI + 5 meest overheenkomende KPI's (syntactisch) <===>\\n\\n\"\n",
    "#     for i in synt_syms:\n",
    "#         content += i + \"\\n\"\n",
    "#         for sym in synt_syms[i]:\n",
    "#             content += f\"- {round(sym[1], 2)}\\t{sym[0]}\\n\"\n",
    "#         content += \"\\n\"\n",
    "        \n",
    "#     file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3af086-18e1-4dfa-a4b0-8939d8c4f734",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sema_syms = dict()\n",
    "# for kpi in tqdm(kpis):\n",
    "#     others = [ i for i in kpis if i != kpi ]\n",
    "#     sema_syms[kpi] = semantic_similarity(kpi, others, 5)\n",
    "    \n",
    "# with open(\"./sema_syms.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "#     content = \"<===> KPI + 5 meest overheenkomende KPI's (semantisch) <===>\\n\\n\"\n",
    "#     for i in sema_syms:\n",
    "#         content += i + \"\\n\"\n",
    "#         for sym in sema_syms[i]:\n",
    "#             content += f\"- {round(sym[1], 2)}\\t{sym[0]}\\n\"\n",
    "#         content += \"\\n\"\n",
    "        \n",
    "#     file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec9fce9-df17-46f2-822c-6c569cf3cde5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rows = []\n",
    "# html_out = widgets.HBox()\n",
    "\n",
    "# for i in zip(synt_syms, sema_syms):\n",
    "#     synt = i[0]\n",
    "#     sema = i[0]\n",
    "    \n",
    "#     grid_layout = widgets.Layout(\n",
    "#         grid_auto_columns=\"min-content\"\n",
    "#     )\n",
    "#     container_layout = widgets.Layout(\n",
    "#         width='auto',\n",
    "#         border=\"2px solid red\",\n",
    "#         flex_flow='col auto'\n",
    "#     )\n",
    "    \n",
    "#     synt_syms_title = widgets.HTML(f\"<h3>{synt}</h3>\")\n",
    "#     synt_syms_score = widgets.VBox([widgets.Label(\"Score:\")] + [widgets.Label(str(round(i[1], 2)), layout=label_layout) for i in synt_syms[synt]])\n",
    "#     synt_syms_phrase = widgets.VBox([widgets.Label(\"KPI:\")] + [widgets.Label(str(i[0]), layout=label_layout) for i in synt_syms[synt]])\n",
    "#     synt_syms_container = widgets.VBox([\n",
    "#         synt_syms_title,\n",
    "#         widgets.HBox([synt_syms_score, synt_syms_phrase], layout=grid_layout)\n",
    "#     ], layout=container_layout)\n",
    "        \n",
    "#     sema_syms_title = widgets.HTML(f\"<h3>{sema}</h3>\")\n",
    "#     sema_syms_score = widgets.VBox([widgets.Label(\"Score:\")] + [widgets.Label(str(round(i[1], 2))) for i in sema_syms[sema]])\n",
    "#     sema_syms_phrase = widgets.VBox([widgets.Label(\"KPI:\")] + [widgets.Label(str(i[0])) for i in sema_syms[sema]])\n",
    "#     sema_syms_container = widgets.VBox([\n",
    "#         sema_syms_title,\n",
    "#         widgets.HBox([sema_syms_score, sema_syms_phrase], layout=grid_layout)\n",
    "#     ], layout=container_layout)\n",
    "    \n",
    "        \n",
    "#     html_out.children += (widgets.HBox([synt_syms_container, sema_syms_container]), )\n",
    "#     break\n",
    "\n",
    "# html_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4596fc-c015-4254-859d-25d2bf3fbced",
   "metadata": {},
   "source": [
    "We hebben nu een lijst met 90 KPI's. Het idee is om ruis KPI's te verwijderen. Wanneer dit gedaan is maken we een functie die alle symantisch/syntactisch overheen komende zinnen uit overheids documenten extrapoleerd welke 90% of meer overheen komen.\n",
    "\n",
    "### Ruis KPI bepaling\n",
    "**Wat maakt een goede KPI om in deze functies te gebruiken?**\n",
    "\n",
    "Een KPI moet het volgende bevatten:\n",
    "- ...\n",
    "\n",
    "Gegeven een KPI, welke chunck komt daar het meest mee overheen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e3676-1d08-4173-8294-82b530d561de",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b67d0aa-b13c-42c0-82d9-54d5cc580bc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T14:57:33.026060Z",
     "start_time": "2023-05-16T14:57:33.008058Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "import PyPDF2\n",
    "\n",
    "def split_pdf_pages(pdf_data):\n",
    "    pages = []\n",
    "    reader = PyPDF2.PdfFileReader(io.BytesIO(pdf_data))\n",
    "\n",
    "    for page_number in range(reader.getNumPages()):\n",
    "        pages.append(reader.getPage(page_number))\n",
    "\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc0c6c82-b981-424a-a747-0af77f5d2e9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T14:57:33.044111Z",
     "start_time": "2023-05-16T14:57:33.025065Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "urls = [\"https://file.notion.so/f/s/49da8fb5-000b-47bf-9788-1dcbdfdf80f6/Omgevingsvisie_Breda_2040_Sterk_en_Veerkrachtig_-_01-07-2021.pdf?id=3be70ed6-2a1a-460a-b326-f0031a4c4d81&table=block&spaceId=ca274338-dfb2-4f65-823d-2acde6195fea&expirationTimestamp=1684312025948&signature=7j13LI3pQfJCwASUOlCiPF8JzLPy6zZmMUcQjkV3xIA&downloadName=Omgevingsvisie+Breda+2040+Sterk+en+Veerkrachtig+-+01-07-2021.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aee986-e05d-4275-987f-9ef19485bc43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T14:58:03.512115Z",
     "start_time": "2023-05-16T14:57:59.087700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kpis = list(handmatige_kpi_collectie.KPI.astype(str))\n",
    "\n",
    "# page_content = []\n",
    "# with open(\"./Definitief concept Omgevingsvisie Weert (20-08-2021).pdf\", \"rb\") as file:\n",
    "#     pdf_reader = PyPDF2.PdfFileReader(file)\n",
    "#     pages = []\n",
    "#     for page_number in tqdm(range(pdf_reader.getNumPages())):\n",
    "#         page = pdf_reader.getPage(page_number)\n",
    "#         page_content.append(page.extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d434f92-ceb3-453a-95b7-051b1c4f4670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrases = re.split(\"\\.|\\n\", \"\\n\".join(page_content))\n",
    "# chuncks = []\n",
    "# N_chunks = 3\n",
    "# while len(phrases) >= N_chunks:\n",
    "#     chunk = \" \".join(phrases[0:N_chunks])\n",
    "#     chuncks.append(chunk)\n",
    "#     phrases = phrases[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960b5b90-235b-442a-9718-b60765d35e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_synt_syms = dict()\n",
    "# for kpi in tqdm(kpis):\n",
    "#     chunk_synt_syms[kpi] = syntactic_similarity(kpi, chuncks, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6b04e4-4d2d-489b-a5a4-6e750b811bd1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in chunk_synt_syms:\n",
    "#     print(i)\n",
    "#     for sim_chunck in chunk_synt_syms[i]:\n",
    "#         print(sim_chunck)\n",
    "#     print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ac0d4-0c57-40af-a2e8-2b2b4c41b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_sema_syms = dict()\n",
    "# for kpi in tqdm(kpis):\n",
    "#     chunk_sema_syms[kpi] = semantic_similarity(kpi, chuncks, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77eec4f-3272-487e-a8d1-aecfca69f382",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in chunk_sema_syms:\n",
    "#     print(i)\n",
    "#     for sim_chunck in chunk_sema_syms[i]:\n",
    "#         print(sim_chunck)\n",
    "#     print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b70bcc-5bb9-4c21-8689-f23d99c8f295",
   "metadata": {},
   "source": [
    "Na aanleiding van bovenstaande uitkomsten kunnen wij concluderen dat het gebruik van semantische overheenkomst meting beter werkt dan syntactische metingen.\n",
    "\n",
    "Er moet wel gekeken worden naar:\n",
    "- andere manier van chunck verwerking (groter maken, minder overlapping etc)\n",
    "- kwalitiet van sommige KPI's is soms twijfelbaar, er moet nader gekeken worden welke handmatig gekozen KPI's nouw echt het beste resultaat geven"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc017042-9873-4d81-a83a-26caa5e1ffbe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1eeccf-92bb-41ac-a011-f8a0fd8bb261",
   "metadata": {},
   "source": [
    "## Pipeline testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44abad85-4a17-4d67-a4a4-8fc0e1491404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 41/41 [00:01<00:00, 31.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# get file content\n",
    "page_content = []\n",
    "with open(\"./Definitief concept Omgevingsvisie Weert (20-08-2021).pdf\", \"rb\") as file:\n",
    "    pdf_reader = PyPDF2.PdfFileReader(file)\n",
    "    pages = []\n",
    "    print(\"Getting page content\")\n",
    "    for page_number in tqdm(range(pdf_reader.getNumPages())):\n",
    "        page = pdf_reader.getPage(page_number)\n",
    "        page_content.append(page.extract_text())\n",
    "        \n",
    "# read KPI collection\n",
    "kpis = list(pd.read_csv(\"handmatige_kpi_collectie.csv\").KPI.astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0ed4f-d76a-4c36-8c16-7e152309494f",
   "metadata": {},
   "source": [
    "To expend the list of KPI's, we measure the most syntactical similair chuncks of text from a given document. Because we don't know up front how many sentences the KPI extends to, we need to take chuncks of text from the document. These chuncks will be groups of N sentences. We have to allow for overlapping chuncks so that we are sure to measure entire KPI's. Then we measure the similairity between the handpicked KPI's and the given chuncks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a01f5df9-1974-4eed-83d2-b4fad0b58496",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d588c7a-0fa2-4de6-bbc9-0ac8eef3ee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals, might need to be tweaked for better results\n",
    "N_chunks = 5\n",
    "kpi_sema_treshhold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61820a4e-d220-48dd-94bc-5defbf496d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_content = \"\\n\\n\".join(page_content) #re.sub(\"[\\.\\n]+\", \" \", )\n",
    "phrases = [ p for p in re.split(\"[\\?\\!\\.\\n]+\", joined_content) if p ]\n",
    "chuncks = []\n",
    "\n",
    "while len(phrases) >= N_chunks:\n",
    "    chunk = \" \".join(phrases[0:N_chunks])\n",
    "    chuncks.append(chunk)\n",
    "    phrases = phrases[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23c7f073-ef7d-4175-a023-e101537da3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████████████████████▏                                                     | 31/90 [07:10<13:38, 13.87s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# store the chuncks which are sementicly similair to a given KPI\n",
    "chunk_sema_syms = dict()\n",
    "\n",
    "for kpi in tqdm(kpis):\n",
    "    sema_chuncks = semantic_similarity(kpi, chuncks)\n",
    "    \n",
    "    # only store if the chunk and the KPI are 90% similair or more\n",
    "    kpi_sema_treshhold = 0.8\n",
    "    chunk_sema_syms[kpi] = [ i for i in sema_chuncks if i[1] >= kpi_sema_treshhold ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c816e24-3561-42b8-a8de-7c54601cc443",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_chuncks = [ str(j[0]) for i in list(chunk_sema_syms.values()) for j in i if i ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7c5aba-3418-4866-be62-0776e138bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in parsed_chuncks:\n",
    "    print(type(i))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb812fbe-dccd-4d00-95e3-1592a91a3606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "vec = TfidfVectorizer(tokenizer=word_tokenize)\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"vec\", vec),\n",
    "    (\"model\", lda)\n",
    "])\n",
    "\n",
    "topic_matrix = pipe.fit_transform(parsed_chuncks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca4c63-f02a-4e03-900d-44817b22ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df = pd.DataFrame(topic_matrix, columns = pipe.get_feature_names_out())\n",
    "lda_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65575bdb-0921-4219-95b1-2a82c47d594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe[\"vec\"].get_feature_names_out()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
